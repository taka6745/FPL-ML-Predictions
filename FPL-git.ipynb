{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJSTYIxCVpLVtkfBQACXib"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6ntRdp2RPeR2"},"outputs":[],"source":["import os\n","import re\n","import pandas as pd\n","from IPython.display import display  # For displaying the DataFrame in Google Colab\n","\n","# Initialize an empty list to store individual DataFrames\n","dfs = []\n","\n","# Initialize a dictionary to store unique columns for each season\n","unique_columns_per_season = {}\n","\n","# Loop through the folders for each season\n","root_dir = '/Users/takoda/Documents/FPL/data/'\n","seasons = [\n","    '2016-17', '2017-18', '2018-19',\n","    '2019-20', '2020-21', '2021-22',\n","    '2022-23', '2023-24'\n","]\n","\n","print(\"Starting to read files...\")\n","\n","for season in seasons:\n","    season_dir = os.path.join(root_dir, season, 'gws')\n","\n","    # Check if the folder exists\n","    if not os.path.exists(season_dir):\n","        print(f\"Skipping missing folder: {season_dir}\")\n","        continue\n","\n","    print(f\"Processing folder: {season_dir}\")\n","\n","    # Initialize set to keep track of unique columns for this season\n","    unique_columns_this_season = set()\n","\n","    # Loop through the files in the folder\n","    for gw_file in os.listdir(season_dir):\n","        # Use regex to filter out unwanted files\n","        if re.match(r'gw\\d+.csv', gw_file):\n","            filepath = os.path.join(season_dir, gw_file)\n","\n","            # Extract gameweek from filename\n","            gameweek = int(gw_file[2:-4])\n","\n","            # Read the data into a DataFrame\n","            try:\n","                df = pd.read_csv(filepath)\n","            except UnicodeDecodeError:\n","                try:\n","                    df = pd.read_csv(filepath, encoding='ISO-8859-1')\n","                except Exception as e:\n","                    print(f\"Could not read {filepath}. Error: {e}\")\n","                    continue\n","            # Update the set of unique columns for this season\n","            unique_columns_this_season.update(df.columns.tolist())\n","\n","            # Add a new column for the season and gameweek\n","            df['season'] = season\n","            df['GW'] = gameweek\n","\n","            # Append to the list of DataFrames\n","            dfs.append(df)\n","\n","    # Store unique columns for this season\n","    unique_columns_per_season[season] = unique_columns_this_season\n","\n","# Concatenate all the DataFrames to form the final DataFrame\n","print(\"Concatenating all DataFrames...\")\n","all_data = pd.concat(dfs, ignore_index=True)\n","\n","# Find similar and unique columns across seasons\n","all_seasons_columns = set.intersection(*(set(x) for x in unique_columns_per_season.values()))\n","unique_columns = {season: cols - all_seasons_columns for season, cols in unique_columns_per_season.items()}\n","\n","print(f\"Similar columns across all seasons: {all_seasons_columns}\")\n","for season, cols in unique_columns.items():\n","    print(f\"Unique columns in season {season}: {cols}\")\n"]},{"cell_type":"code","source":["# Keep only the columns that are similar across all seasons, plus the \"Season\" column\n","final_columns = list(all_seasons_columns) + ['season'] + ['GW']\n","all_data = all_data[final_columns]"],"metadata":{"id":"Ho97gVJfW2Rk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify the columns to be removed\n","columns_to_remove = [col for col in all_data.columns if col not in final_columns]\n","\n","# Remove those columns\n","if columns_to_remove:  # Only proceed if there are columns to remove\n","    all_data.drop(columns=columns_to_remove, inplace=True)\n"],"metadata":{"id":"H_rrP15TaGR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming all_data is your DataFrame\n","all_data = all_data.drop(['kickoff_time', 'team_h_score', 'fixture', 'team_a_score', 'round'], axis=1)"],"metadata":{"id":"C3zeNdiDe3Gf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert all columns except 'name', 'Season', and 'was_home' to numeric\n","cols_to_convert = [col for col in all_data.columns if col not in ['name', 'season', 'was_home']]\n","all_data[cols_to_convert] = all_data[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n","\n","# Convert 'was_home' to boolean\n","all_data['was_home'] = all_data['was_home'].astype(bool)"],"metadata":{"id":"YJeboJM8fBF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming all_data is your DataFrame\n","all_data.to_csv('all_data.csv', index=False)\n"],"metadata":{"id":"RF0GckhSfsOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import numpy as np\n","\n","# Display summary statistics\n","print(all_data.describe())\n","\n","# Plotting numeric features\n","numeric_features = all_data.select_dtypes(include=[np.number]).columns.tolist()\n","\n","# Correlation Matrix Heatmap\n","corrmat = all_data[numeric_features].corr()\n","plt.figure(figsize=(12, 9))\n","sns.heatmap(corrmat, vmax=.8, square=True, annot=True, fmt='.2f', cmap='coolwarm')\n","\n","# Count of NaN values for each column\n","missing_values_count = all_data.isnull().sum()\n","missing_values_count = missing_values_count[missing_values_count > 0]\n","missing_values_count.sort_values(inplace=True)\n","\n","if not missing_values_count.empty:\n","    plt.figure(figsize=(10, 6))\n","    sns.barplot(x=missing_values_count.index, y=missing_values_count.values, palette='viridis')\n","    plt.title('Missing Values Count by Column')\n","    plt.xlabel('Columns')\n","else:\n","    print(\"No missing values to plot.\")\n","\n","\n","plt.show()"],"metadata":{"id":"YtwKbDDceZnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assume `df` is your main DataFrame and `team_df` is loaded from 'master_team_list.csv'\n","team_df = pd.read_csv(os.path.join(root_dir, 'master_team_list.csv'))\n","\n","# Merge to get opponent_team_name\n","all_data = pd.merge(all_data, team_df, left_on=['season', 'opponent_team'], right_on=['season', 'team'], how='left')\n","\n","# Assuming df is your DataFrame\n","\n","# List of columns to compute rolling averages for\n","cols_to_average = [\n","    \"bonus\", \"transfers_balance\", \"ict_index\",\n","    \"penalties_saved\", \"opponent_team\", \"minutes\", \"clean_sheets\",\n","    \"value\", \"was_home\", \"saves\", \"transfers_in\", \"influence\",\n","    \"penalties_missed\", \"assists\", \"goals_conceded\", \"threat\",\n","    \"own_goals\", \"transfers_out\", \"red_cards\", \"goals_scored\",\n","    \"bps\", \"creativity\", \"selected\", \"yellow_cards\"\n","]\n","\n","# Create rolling averages (over the last 5 game weeks for instance)\n","for col in cols_to_average:\n","    all_data[f\"{col}_rolling_5\"] = all_data.groupby(\"element\")[col].transform(lambda x: x.rolling(window=5, min_periods=1).mean().shift(1))\n","    all_data[f\"{col}_rolling_10\"] = all_data.groupby(\"element\")[col].transform(lambda x: x.rolling(window=10, min_periods=1).mean().shift(1))\n","all_data.reset_index(inplace=True)\n","# Loop through each player and opponent to calculate rolling averages for 5 and 10 GWs\n","for player in all_data['element'].unique():\n","    player_indices = all_data.index[all_data['element'] == player]\n","\n","    for opponent in all_data.loc[player_indices, 'opponent_team'].unique():\n","        opponent_indices = all_data.index[(all_data['element'] == player) & (all_data['opponent_team'] == opponent)]\n","\n","        # Sort by 'GW'\n","        # Sort by 'GW' within each 'element'\n","        opponent_indices = opponent_indices.sortlevel(level='GW')[0]\n","\n","\n","        # Compute the rolling averages for 'total_points' against this specific opponent\n","        total_points_rolling_5 = all_data.loc[opponent_indices, 'total_points'].rolling(window=5, min_periods=1).mean().shift(1)\n","        total_points_rolling_10 = all_data.loc[opponent_indices, 'total_points'].rolling(window=10, min_periods=1).mean().shift(1)\n","\n","        # Update the original all_data DataFrame\n","        all_data.loc[opponent_indices, 'total_opp_points_rolling_5'] = total_points_rolling_5.values\n","        all_data.loc[opponent_indices, 'total_opp_points_rolling_10'] = total_points_rolling_10.values\n","\n","# Now, all_data should have the new columns 'total_opp_points_rolling_5' and 'total_opp_points_rolling_10'\n"],"metadata":{"id":"AQLkbaz4nfgP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Big Data creation"],"metadata":{"id":"YPbwElsRJQhR"}},{"cell_type":"code","source":["all_columns = all_data.columns.tolist()\n","static_columns = ['name', 'element', 'season', 'GW', 'team', 'team_name', 'opponent_team', 'was_home']\n","dynamic_columns = [col for col in all_columns if col not in static_columns]  # Include 'total_points'\n","\n","# Create lagged features for dynamic columns\n","for col in dynamic_columns:\n","    all_data[f'lagged_{col}'] = all_data.groupby('name')[col].shift(1)\n"],"metadata":{"id":"oW1vy3TMkCPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_data['lagged_total_points_rolling_5'] = all_data.groupby('name')['lagged_total_points'].transform(lambda x: x.rolling(window=5, min_periods=1).mean().shift(1))\n","all_data['lagged_total_points_rolling_10'] = all_data.groupby('name')['lagged_total_points'].transform(lambda x: x.rolling(window=10, min_periods=1).mean().shift(1))"],"metadata":{"id":"lOEDgTCnkXQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import combinations\n","import numpy as np\n","import pandas as pd\n","\n","from itertools import combinations\n","\n","def create_interaction_terms(df, columns, max_order=2):\n","    all_interactions = {}\n","\n","    for order in range(2, max_order + 1):  # From 2-way to max_order-way interactions\n","        all_combinations = combinations(columns, order)\n","        for combo in all_combinations:\n","            col_name = '_x_'.join(combo)\n","            all_interactions[col_name] = np.prod(df[list(combo)], axis=1)\n","\n","    return pd.DataFrame(all_interactions)\n","\n","def create_polynomial_and_other_features(df, column):\n","    polynomial_features = {\n","        f'{column}_squared': df[column] ** 2,\n","        f'{column}_cubed': df[column] ** 3,\n","        f'{column}_sqrt': np.sqrt(df[column]),\n","        f'{column}_cbrt': np.cbrt(df[column]),\n","        f'{column}_log': np.log(df[column] + 1)  # Adding 1 to avoid log(0)\n","    }\n","    return pd.DataFrame(polynomial_features)\n","\n","\n","# Select all columns, don't filter out 'rolling' ones\n","lagged_cols = ['lagged_' + col for col in dynamic_columns if 'rolling' not in col]\n","\n","\n","\n","\n","# Concatenate all\n","all_data = pd.concat([all_data, interaction_df, polynomial_df], axis=1)\n"],"metadata":{"id":"cIHU7Wfiz59t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop or fill missing values\n","all_data.dropna(inplace=True)  # or data_with_const.fillna(0, inplace=True)"],"metadata":{"id":"qxOtll0ay6Zv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check data types\n","print(all_data.dtypes)\n"],"metadata":{"id":"GLDULEx0zFKo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save to CSV so we only have to do this once"],"metadata":{"id":"LbDH6CsiJSFy"}},{"cell_type":"code","source":["# Your allowed list of column names\n","allowed_columns = [\"name\", \"element\", \"opponent_team\", \"was_home\", \"season\", \"GW\", \"team_name\"]\n","\n","# Create a list of columns that are either in allowed_columns or have 'lagged' in their name\n","columns_to_keep = [col for col in all_data.columns if col in allowed_columns or 'lagged' in col]\n","\n","# Drop all other columns\n","all_data_filtered = all_data[columns_to_keep]\n"],"metadata":{"id":"Pd1HM9ZU3f5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop specified columns\n","columns_to_drop = [\"value\", \"influence\", \"ict_index\", \"creativity\", \"threat\",\n","                   \"bonus\", \"bps\", \"minutes\", \"clean_sheets\", \"goals_scored\", \"assists\"]\n","\n","all_data = all_data.drop(columns=columns_to_drop)\n","\n","# Now, all_data will not have the specified columns.\n"],"metadata":{"id":"MBq4JnIMHxKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","all_data.to_csv('all_data.csv', index=False)"],"metadata":{"id":"o00y8-2yPLV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in all_data.columns:\n","  print(i)"],"metadata":{"id":"pxCxAFUdxA-m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e3uzevbu5it8"},"execution_count":null,"outputs":[]}]}